{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment - shallow learning\n",
    "\n",
    "Hi there! In this assignment, you will use shallow learning (including svm, random forests and gradient boosting if you feel up for the challenge) to solve an adapted Question 1 of the winter 2023 exam in applied machine learning:\n",
    "\n",
    "## Introduction:\n",
    "\n",
    "During the semester you have become very excited about the field of digital pathology which is an area that is developing rapidly due to advancements in microscopy imaging hardware. These advancements have allowed digitizing glass slides into whole-slide images. You have recently read the paper by [Veeling et al (2018)](https://arxiv.org/abs/1806.03962) and you are thrilled to see that the authors have derived a novel dataset, denoted PatchCamelyon (PCam), that will enable you to develop and benchmark your own machine learning models. As Veeling et al (2018) you are primarily interested in developing machine learning models that based on patches of whole-slide images of lymph node sections can assist pathologist in tumor detection. \n",
    "\n",
    "The primary objective of this exam is to perform image classification using the PCam dataset. The full dataset consists of 327,680 color images (96x96pxs) extracted from histopathologic scans of lymph node sections. Each image is annotated with a binary label indicating presence of metastatic tissue. For this assignment, however, you are only going to use the subset of the data which have been made available on Kaggle.\n",
    "\n",
    "### Question 1 (adapted from the exam):\n",
    "Use non-deep learning to perform image classification (tumor detection). Consider among other things the following:\n",
    "1. Support vector machines\n",
    "2. Random forests\n",
    "3. Boosting\n",
    "4. A combination of two or all three of the methods\n",
    "5. Assess the importance of image resolution for the methods you are using\n",
    "\n",
    "The assignment is posted as a Kaggle competition and is available here: https://www.kaggle.com/t/1f880200648443a3a30878d318cc6e4b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hints to get you started (with a very simple model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function that grayscale, resize and flattens the image. This function might also become handy (for deep learning later) if the original images are too large for your hardware configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sample(image):\n",
    "    image = tf.image.rgb_to_grayscale(image)\n",
    "    image = tf.image.resize(image,[32,32]).numpy()\n",
    "    image = image.reshape(1,-1)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\amlfall23\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:591: FutureWarning: Passing an int for a boolean parameter is deprecated in version 1.2 and won't be supported anymore in version 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data features (observations,features): (26214, 1024)\n",
      "Shape of training data labels (observations,): (26214,)\n",
      "Shape of training data features (observations,features): (1638, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\amlfall23\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:591: FutureWarning: Passing an int for a boolean parameter is deprecated in version 1.2 and won't be supported anymore in version 1.4.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X = np.load('C:/Users/thorp/OneDrive/Dokumenter/Uni/Kandidat/Anvendt maskinlæring/Xtrain.npy/Xtrain.npy')\n",
    "X = np.vstack(list(map(convert_sample,X)))\n",
    "X = StandardScaler(with_mean=0, with_std=1).fit_transform(X)\n",
    "print(f'Shape of training data features (observations,features): {X.shape}')\n",
    "\n",
    "y = np.load('C:/Users/thorp/OneDrive/Dokumenter/Uni/Kandidat/Anvendt maskinlæring/ytrain.npy')\n",
    "y = y.reshape(-1,)    \n",
    "print(f'Shape of training data labels (observations,): {y.shape}')\n",
    "\n",
    "Xtest = np.load('C:/Users/thorp/OneDrive/Dokumenter/Uni/Kandidat/Anvendt maskinlæring/Xtest.npy/Xtest.npy')\n",
    "Xtest = np.vstack(list(map(convert_sample,Xtest)))\n",
    "Xtest = StandardScaler(with_mean=0, with_std=1).fit_transform(Xtest)\n",
    "print(f'Shape of training data features (observations,features): {Xtest.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is then ready to be applied for training and prediction in a shallow learning model such as the SVM classifier...below just a very very simple illustration on how to construct and train a support vector machine based on the data we have prepared. The predicted file can be submitted to Kaggle for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nclf = svm.LinearSVC(max_iter=100000)\\nclf.fit(X_train, y_train)\\ny_test_hat = clf.predict(X_test)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "''' \n",
    "clf = svm.LinearSVC(max_iter=100000)\n",
    "clf.fit(X_train, y_train)\n",
    "y_test_hat = clf.predict(X_test)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6202555788670608\n"
     ]
    }
   ],
   "source": [
    "accuracy_linear_int = accuracy_score(y_test_hat, y_test)\n",
    "print(accuracy_linear_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest_hat = pd.DataFrame({\n",
    "    'Id': list(range(len(y_test_hat))),\n",
    "    'Predicted': y_test_hat.reshape(-1,),\n",
    "})\n",
    "ytest_hat.to_csv('C:/Users/computer/Documents/AML folder/AML-Code/AML Code/Assignments/test_hat.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple hyperparameter search found no better than base case\n",
    "Kernel = [\"linear\", \"rbf\"] \n",
    "Cs = [0.1, 0.5, 1, 2, 3, 100, 1000]\n",
    "\n",
    "results = []\n",
    "\n",
    "for kernel in Kernel:\n",
    "    for C in Cs:\n",
    "        svm_poly = svm.SVC(kernel=kernel, C=C, decision_function_shape='ovr')\n",
    "        svm_poly.fit(X_train, y_train)\n",
    "        y_val_hat = svm_poly.predict(X_test)\n",
    "        accuracy = accuracy_score(y_val_hat, y_test)\n",
    "\n",
    "        results.append([accuracy, kernel, C])\n",
    "\n",
    "results = pd.DataFrame(results)\n",
    "results.columns = ['Accuracy', 'Polynomial degree', 'C']\n",
    "print(results)\n",
    "\n",
    "# Extract best parameters.\n",
    "#results[results['Accuracy'] == results['Accuracy'].max()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now move on to decision trees i saw no better case than the base case of rbf in the SVM also hyperparameter seach is difficult, maybe i should do some PCA given enough computational power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT with default settings achieved 64.8% accuracy with Depth: 59.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from matplotlib import pyplot as plt\n",
    "# Initialize a DT (default setting)\n",
    "dt_default = tree.DecisionTreeClassifier()\n",
    "\n",
    "\n",
    "# Fit your DT\n",
    "dt_default.fit(X_train, y_train)\n",
    "\n",
    "# Predict on your test data with your DT\n",
    "y_test_hat_default = dt_default.predict(X_test) \n",
    "\n",
    "# Obtain accuracy by using the `accuracy_score` function\n",
    "accuracy_default = accuracy_score(y_test_hat_default, y_test)\n",
    "\n",
    "# Print results\n",
    "print(f'DT with default settings achieved {round(accuracy_default * 100, 1)}% accuracy with Depth: {dt_default.get_depth()}.')\n",
    "#Basic model gained 64.8% with basic settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic model gained 64.8% with basic settings, i will try to tune it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT with default settings achieved 65.4% accuracy with Depth: 63.\n"
     ]
    }
   ],
   "source": [
    "max_depth = 5 # try more values than just 5 here! Also try fractions!\n",
    "\n",
    "# Initialize DT\n",
    "dt_low_max_depth = tree.DecisionTreeClassifier(max_depth=max_depth)\n",
    "\n",
    "# Fit your DT\n",
    "dt_default.fit(X_train, y_train)\n",
    "\n",
    "# Predict on your test data with your DT\n",
    "y_test_hat_default = dt_default.predict(X_test) \n",
    "\n",
    "# Obtain accuracy by using the `accuracy_score` function\n",
    "accuracy_default = accuracy_score(y_test_hat_default, y_test)\n",
    "\n",
    "# Print results\n",
    "print(f'DT with default settings achieved {round(accuracy_default * 100, 1)}% accuracy with Depth: {dt_default.get_depth()}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max depth 5 gave 65.4% accuracy now we loop  it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Max Depth  Accuracy\n",
      "0           1  64.88652\n",
      "1           2  64.31432\n",
      "2           3  65.09632\n",
      "3           4  64.46691\n",
      "4           5  64.56227\n",
      "5           6  64.58135\n",
      "6           7  65.09632\n",
      "7           8  64.96281\n",
      "8           9  64.73393\n",
      "9          10  65.07725\n",
      "10         11  64.81022\n",
      "11         12  64.92466\n",
      "12         13  65.34427\n",
      "13         14  65.00095\n",
      "14         15  65.00095\n",
      "15         16  64.79115\n",
      "16         17  65.05817\n",
      "17         18  65.93553\n",
      "18         19  65.05817\n",
      "19         20  65.32520\n",
      "20         21  64.96281\n",
      "21         22  65.26798\n",
      "22         23  65.26798\n",
      "23         24  64.98188\n",
      "24         25  64.84837\n",
      "25         26  65.26798\n",
      "26         27  64.77208\n",
      "27         28  65.30612\n",
      "28         29  64.69578\n",
      "29         30  65.09632\n"
     ]
    }
   ],
   "source": [
    "n = 30\n",
    "max_depth = [i for i in range(1, n+1)]\n",
    "\n",
    "results = []\n",
    "\n",
    "for max_depth in max_depth:\n",
    "    dt_low_max_depth = tree.DecisionTreeClassifier(max_depth=max_depth)\n",
    "    dt_default.fit(X_train, y_train)\n",
    "    y_test_hat_default = dt_default.predict(X_test) \n",
    "    accuracy = accuracy_score(y_test_hat_default, y_test)\n",
    "    \n",
    "    results.append([max_depth,round(accuracy * 100, 5)])\n",
    "\n",
    "results = pd.DataFrame(results)\n",
    "results.columns = ['Max Depth',\n",
    "                   'Accuracy']\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Max Depth</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>65.93553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Max Depth  Accuracy\n",
       "17         18  65.93553"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[results['Accuracy'] == results['Accuracy'].max()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From max_depth 1-30 we get that the depth of 18 gives 65.9, i will try other params and then try PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "max_depth_values = [4, 5, 15, 17]\n",
    "min_samples_split_values = [3, 5, 7, 10]\n",
    "min_samples_leaf_values = [3, 5, 7, 10]\n",
    "\n",
    "results = []\n",
    "\n",
    "for max_depth in max_depth_values:\n",
    "    for min_samples_split in min_samples_split_values:\n",
    "        for min_samples_leaf in min_samples_leaf_values:\n",
    "            dt_model = DecisionTreeClassifier(max_depth=max_depth, \n",
    "                                             min_samples_split=min_samples_split, \n",
    "                                             min_samples_leaf=min_samples_leaf)\n",
    "            dt_model.fit(X_train, y_train)\n",
    "            y_test_hat = dt_model.predict(X_test)\n",
    "            accuracy = accuracy_score(y_test_hat, y_test)\n",
    "            \n",
    "            results.append([max_depth, min_samples_split, min_samples_leaf, accuracy])\n",
    "\n",
    "results = pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Max Depth  Min Sample Split  Min sample_leaf  Accuracy\n",
      "0           4                 3                3  0.647149\n",
      "1           4                 3                5  0.647149\n",
      "2           4                 3                7  0.647149\n",
      "3           4                 3               10  0.647149\n",
      "4           4                 5                3  0.647149\n",
      "..        ...               ...              ...       ...\n",
      "59         17                 7               10  0.678810\n",
      "60         17                10                3  0.670799\n",
      "61         17                10                5  0.674423\n",
      "62         17                10                7  0.679954\n",
      "63         17                10               10  0.676140\n",
      "\n",
      "[64 rows x 4 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Max Depth</th>\n",
       "      <th>Min Sample Split</th>\n",
       "      <th>Min sample_leaf</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.679954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Max Depth  Min Sample Split  Min sample_leaf  Accuracy\n",
       "62         17                10                7  0.679954"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "results.columns = ['Max Depth', \n",
    "                   'Min Sample Split',\n",
    "                   'Min sample_leaf',\n",
    "                   'Accuracy']\n",
    "print(results)\n",
    "results[results['Accuracy'] == results['Accuracy'].max()]\n",
    "                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "We achived a accuracy of 0.6799 with max depth 17 plit of 10 and min_leaf of 7, i will now run a super long model\n",
    "\n",
    "I will now do a grid seach for the hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "enable_halving_search_cv=True\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "param_grid = {\n",
    "    'max_depth': [4, 5, 15, 17],\n",
    "    'min_samples_split': [3, 5, 7, 10],\n",
    "    'min_samples_leaf': [3, 5, 7, 10, 12, 15, 16]\n",
    "}\n",
    "\n",
    "dt_model = DecisionTreeClassifier()\n",
    "grid_search = HalvingGridSearchCV(dt_model, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = grid_search.cv_results_\n",
    "for mean_score, params in zip(results['mean_test_score'], results['params']):\n",
    "    print(\"Accuracy: {:.5f}, Hyperparameters: {}\".format(mean_score, params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing good came of this, we try the nuclear option, random forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import ensemble  # ensemble instead of tree\n",
    "# Initialize\n",
    "rf = ensemble.RandomForestClassifier(criterion=\"entropy\")\n",
    "\n",
    "# Fit\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "y_test_hat = rf.predict(Xtest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26214,)\n",
      "(1638,)\n"
     ]
    }
   ],
   "source": [
    "# accuracy = accuracy_score(y_test_hat, y_test)\n",
    "# print(accuracy)\n",
    "print(y.shape)\n",
    "print(y_test_hat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic random just did better with a score of 76.4%, minimizing for enthropy gives us a score of 77,9 lets do some parameter search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}\n"
     ]
    }
   ],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = [2, 4, 6] #Must be int we are not doing regression, we are doing image classification\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "#Initiate the random forest\n",
    "\n",
    "rf = ensemble.RandomForestClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores (for a general test i do 3x3 to see what happens)\n",
    "rf_random = RandomizedSearchCV(estimator = rf, \n",
    "                               param_distributions = random_grid, \n",
    "                               n_iter = 3, cv = 3, \n",
    "                               verbose=2, \n",
    "                               random_state=42, \n",
    "                               n_jobs = -1)\n",
    "y_test_hat_rs = rf_random.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1638, 2)\n"
     ]
    }
   ],
   "source": [
    "ytest_hat = pd.DataFrame({\n",
    "    'Id': list(range(len(y_test_hat))),\n",
    "    'Predicted': y_test_hat.reshape(-1,),\n",
    "})\n",
    "ytest_hat.to_csv('C:/Users/thorp/Downloads/test_hat.csv', index=False)\n",
    "print(ytest_hat.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
