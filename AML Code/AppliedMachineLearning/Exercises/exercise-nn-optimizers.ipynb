{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Time to experiment with activation functions and optimizers! And now we're at it, let's use this as an introduction to regression using neural networks as well.\n",
    "\n",
    "1. Use the **fetch_california_housing** data (remember to split your data into a train and test data). Use the five optimizers presented in class to train five neural networks (identival aside from the optimizer used). How well does the networks perform on the test set, as measured by MSE and MAE? Rank the optimizers.\n",
    "1. Select the best optimizer and use it for this exercise. Experiment with different activation functions, including at least sigmoid, tanh, and relu. Rank the activation functions you try. \n",
    "1. Using your findings, as well as experimenting with more layers, try to minimize the test MSE.\n",
    "\n",
    "**Note**: You may want to use https://www.tensorflow.org/api_docs/python/tf/keras/activations and https://www.tensorflow.org/api_docs/python/tf/keras/optimizers.\n",
    "\n",
    "**See slides for more details!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "Use the **fetch_california_housing** data (remember to split your data into a train and test data). Use the five optimizers presented in class to train five neural networks (identival aside from the optimizer used). How well does the networks perform on the test set, as measured by MSE and MAE? Rank the optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16512, 8) (4128, 8) (16512,) (4128,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "x, y = fetch_california_housing(return_X_y=True)\n",
    "\n",
    "# Use `train_test_split` to split your data into a train and a test set.\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "z_train = scaler.fit_transform(x_train)\n",
    "z_test = scaler.transform(x_test)\n",
    "\n",
    "print(z_train.shape, z_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a small function you can use as a starting point for your network - but feel free to experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nn(activation = 'sigmoid'):\n",
    "    your_regression_nn = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation=activation, input_shape=(8,)), # input_shape=8 since 8 features\n",
    "        tf.keras.layers.Dense(1, activation='linear'), # linear is used for regression. 1 node since 1 output (pr. observation)\n",
    "        ])\n",
    "\n",
    "    return your_regression_nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important note**: Remember to use \"mse\" as your loss function! Now, it is okay to try something else, but at least do not use cross entropy (remember that is for classification.\n",
    "\n",
    "Go through each of the five optimizers covered in class and rank their performance on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.8502 - mae: 0.6902\n",
      "Epoch 2/5\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5868 - mae: 0.5690\n",
      "Epoch 3/5\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.5518 - mae: 0.5481\n",
      "Epoch 4/5\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.5298 - mae: 0.5360\n",
      "Epoch 5/5\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5158 - mae: 0.5283\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 0.5100 - mae: 0.5140\n",
      "Epoch 1/5\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 1.3885 - mae: 0.8363\n",
      "Epoch 2/5\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5387 - mae: 0.5440\n",
      "Epoch 3/5\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.4840 - mae: 0.5096\n",
      "Epoch 4/5\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.4690 - mae: 0.5007\n",
      "Epoch 5/5\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.4609 - mae: 0.4958\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 0.4625 - mae: 0.4863\n",
      "Epoch 1/5\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 1.9230 - mae: 1.0095\n",
      "Epoch 2/5\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 1.1616 - mae: 0.7911\n",
      "Epoch 3/5\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 1.0171 - mae: 0.7667\n",
      "Epoch 4/5\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.9569 - mae: 0.7577\n",
      "Epoch 5/5\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.9183 - mae: 0.7470\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 0.8799 - mae: 0.7381\n",
      "Epoch 1/5\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 3.6171 - mae: 1.5051\n",
      "Epoch 2/5\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 3.5018 - mae: 1.4702\n",
      "Epoch 3/5\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 3.3701 - mae: 1.4302\n",
      "Epoch 4/5\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 3.2294 - mae: 1.3876\n",
      "Epoch 5/5\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 3.0852 - mae: 1.3436\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 2.8814 - mae: 1.2832\n",
      "     mse    mae optimizer\n",
      "0  0.510  0.514       SGD\n",
      "1  0.462  0.486      Adam\n",
      "2  0.880  0.738   Adagrad\n",
      "3  2.881  1.283  Adadelta\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "opti = ['SGD', 'Adam', 'Adagrad', 'Adadelta']\n",
    "results = []\n",
    "# SGD\n",
    "# This code I have completed for you - use it to construct to other 4 cases (i.e. for the other 4 optimizers covered in class).\n",
    "for optimizer in opti:\n",
    "    nn_sgd = build_nn()\n",
    "    nn_sgd.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='mse',\n",
    "        metrics=['mae'], # to also track MAE. MSE is \"automatically\" measured since it is the loss\n",
    "        )\n",
    "    nn_sgd.fit(z_train, y_train, epochs=5)\n",
    "    mse, mae = nn_sgd.evaluate(z_test, y_test)\n",
    "    \n",
    "    results.append([round(mse, 3), round(mae, 3), optimizer])\n",
    "\n",
    "results = pd.DataFrame(results)\n",
    "results.columns = ['mse', 'mae','optimizer']\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam has the lowest MSE and MAE and is therefore better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "Select the best optimizer and use it for this exercise. Experiment with different activation functions, including at least sigmoid, tanh, and relu. Rank the activation functions you try. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 1.0199 - mae: 0.7284\n",
      "Epoch 2/5\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5156 - mae: 0.5293\n",
      "Epoch 3/5\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.4780 - mae: 0.5074\n",
      "Epoch 4/5\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.4673 - mae: 0.5004\n",
      "Epoch 5/5\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.4594 - mae: 0.4954\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 0.4617 - mae: 0.4878\n",
      "Epoch 1/5\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 1.1857 - mae: 0.7605\n",
      "Epoch 2/5\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.4729 - mae: 0.5029\n",
      "Epoch 3/5\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.4626 - mae: 0.4972\n",
      "Epoch 4/5\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.4570 - mae: 0.4947\n",
      "Epoch 5/5\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.4499 - mae: 0.4908\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 0.4641 - mae: 0.4915\n",
      "Epoch 1/5\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 1.0971 - mae: 0.7379\n",
      "Epoch 2/5\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.4640 - mae: 0.4894\n",
      "Epoch 3/5\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.4116 - mae: 0.4584\n",
      "Epoch 4/5\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.3984 - mae: 0.4452\n",
      "Epoch 5/5\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.3765 - mae: 0.4369\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 0.3875 - mae: 0.4316\n",
      "     mse    mae optimizer Activator\n",
      "0  0.462  0.488  Adadelta   sigmoid\n",
      "1  0.464  0.491  Adadelta      tanh\n",
      "2  0.387  0.432  Adadelta      relu\n"
     ]
    }
   ],
   "source": [
    "# Example of using tanh\n",
    "acti = ['sigmoid', 'tanh', 'relu']\n",
    "results = []\n",
    "# SGD\n",
    "# This code I have completed for you - use it to construct to other 4 cases (i.e. for the other 4 optimizers covered in class).\n",
    "for act in acti:\n",
    "    nn_sgd = build_nn(act)\n",
    "    nn_sgd.compile(\n",
    "        optimizer='Adam',\n",
    "        loss='mse',\n",
    "        metrics=['mae'], # to also track MAE. MSE is \"automatically\" measured since it is the loss\n",
    "        )\n",
    "    nn_sgd.fit(z_train, y_train, epochs=5)\n",
    "    mse, mae = nn_sgd.evaluate(z_test, y_test)\n",
    "    \n",
    "    results.append([round(mse, 3), round(mae, 3), optimizer, act])\n",
    "\n",
    "results = pd.DataFrame(results)\n",
    "results.columns = ['mse', 'mae','optimizer', 'Activator']\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "Using your findings, as well as experimenting with more layers, try to minimize the test MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to experiment a bit, but here is an example of a model with more layers\n",
    "def build_better_nn(activation):\n",
    "    your_regression_nn = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(32, activation=activation, input_shape=(8,)), # input_shape=8 since 8 features\n",
    "        tf.keras.layers.Dense(64, activation=activation),\n",
    "        tf.keras.layers.Dense(128, activation=activation),\n",
    "        tf.keras.layers.Dense(1, activation='linear'), # linear is used for regression. 1 node since 1 output\n",
    "        ])\n",
    "\n",
    "    return your_regression_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_final = build_better_nn(??)\n",
    "nn_final.compile(\n",
    "    optimizer=??,\n",
    "    loss='mse',\n",
    "    metrics=['mae'],\n",
    "    )\n",
    "nn_final.fit(z_train, y_train, epochs=??)\n",
    "mse, mae = nn_final.evaluate(z_test, y_test)\n",
    "print(f'Final model test mse = {round(mse, 3)}, test mae = {round(mae, 3)}.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
